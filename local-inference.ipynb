{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a0f74f",
   "metadata": {},
   "source": [
    "# Deepfake Audio Detection: Local Inference\n",
    "\n",
    "This notebook demonstrates how to run local inference on a fine-tuned Wav2Vec2 model for deepfake detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ae914",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Install required dependencies and configure the environment. This includes PyTorch with CUDA support, Transformers, and audio processing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d516d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --extra-index-url https://download.pytorch.org/whl/cu128 torch torchcodec torchaudio xformers -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333sbpbfd63",
   "metadata": {},
   "source": [
    "## Step 2: Simple Local Inference\n",
    "\n",
    "This section shows how to load the fine-tuned model from Hugging Face and run inference on your own audio files. Perfect for users who just want to test the model without going through the full training pipeline.\n",
    "\n",
    "**Requirements:**\n",
    "- Audio file (mp3, wav, or flac format)\n",
    "- 16kHz sample rate recommended (will auto-convert if different)\n",
    "\n",
    "**Quick start:**\n",
    "1. Replace `audio_file_path` with your audio file\n",
    "2. Run the cell\n",
    "3. Get prediction: real or fake audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from Hugging Face Hub (Python equivalent of: hf download <repo_id>)\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"garystafford/wav2vec2-deepfake-voice-detector\"\n",
    "\n",
    "# Pick a local directory inside the repo (change if you prefer a different location)\n",
    "local_dir = Path(\"model_download\") / REPO_ID.replace(\"/\", \"__\")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import snapshot_download\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"Missing dependency 'huggingface_hub'. Install it with: %pip install huggingface_hub\"\n",
    "    ) from e\n",
    "\n",
    "download_path = snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    local_dir=str(local_dir),\n",
    "    local_dir_use_symlinks=False,\n",
    "    resume_download=True,\n",
    "    allow_patterns=[\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\", \"*.model\", \"*.py\"],\n",
    "    ignore_patterns=[\"*.md\"],\n",
    "    revision=\"main\",\n",
    "    cache_dir=None,\n",
    "    token=None,  # set to True / token string if the repo is private\n",
    ")\n",
    "\n",
    "print(f\"Downloaded '{REPO_ID}' to: {download_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5143014",
   "metadata": {},
   "source": [
    "### Local Inference: Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lhla69bz6uj",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load the fine-tuned model from Hugging Face\n",
    "# ============================================================================\n",
    "model_name_base = \"Gustking/wav2vec2-large-xlsr-deepfake-audio-classification\"\n",
    "model_name_ft = \"garystafford/wav2vec2-deepfake-voice-detector\"\n",
    "model_name = model_name_ft  # change to model_name_base to use base model\n",
    "\n",
    "print(\"Loading model from Hugging Face...\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Model loaded on: {device}\\n\")\n",
    "\n",
    "\n",
    "THRESHOLD = 0.40\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Define inference function\n",
    "# ============================================================================\n",
    "def predict_audio(audio_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict if audio is real or fake.\n",
    "\n",
    "    Args:\n",
    "        audio_path: Path to audio file (mp3, wav, flac)\n",
    "        threshold: Decision threshold (default 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with prediction and probabilities\n",
    "    \"\"\"\n",
    "    # Load audio file at 16kHz\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "\n",
    "    # Extract features\n",
    "    inputs = feature_extractor(\n",
    "        audio, sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "\n",
    "    # Move to same device as model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    prob_real = probs[0][0].item()\n",
    "    prob_fake = probs[0][1].item()\n",
    "\n",
    "    # Make prediction based on threshold\n",
    "    prediction = \"fake\" if prob_fake >= threshold else \"real\"\n",
    "    confidence = prob_fake if prediction == \"fake\" else prob_real\n",
    "\n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\"real\": prob_real, \"fake\": prob_fake},\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Test audio files\n",
    "# ============================================================================\n",
    "\n",
    "audio_files = (\n",
    "    glob.glob(\"audio_samples/*.flac\")\n",
    "    + glob.glob(\"audio_samples/*.wav\")\n",
    "    + glob.glob(\"audio_samples/*.mp3\")\n",
    ")\n",
    "print(f\"\\n\\nTesting {len(audio_files)} files:\\n\")\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    result = predict_audio(audio_file, threshold=THRESHOLD)\n",
    "    pred_symbol = \"ðŸ”´\" if result[\"prediction\"] == \"fake\" else \"ðŸŸ¢\"\n",
    "    print(\n",
    "        f\"{pred_symbol} {audio_file.split('/')[-1]:40s} â†’ {result['prediction']:5s} ({result['confidence']:.1%})\"\n",
    "    )\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44512a7f",
   "metadata": {},
   "source": [
    "### Local Inference Code from Blog Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b227ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "import librosa\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "model_id = \"garystafford/wav2vec2-deepfake-voice-detector\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def predict_audio(audio_path, threshold=0.4):\n",
    "    # Load and resample to 16kHz\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "\n",
    "    # Extract features\n",
    "    inputs = feature_extractor(\n",
    "        audio, sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert to probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    prob_real = probs[0][0].item()\n",
    "    prob_fake = probs[0][1].item()\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = \"fake\" if prob_fake >= threshold else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"confidence\": max(prob_real, prob_fake),\n",
    "        \"probabilities\": {\"real\": prob_real, \"fake\": prob_fake},\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on audio file\n",
    "result = predict_audio(\"sample_audio.mp3\", threshold=0.40)\n",
    "print(f\"Prediction: {result['prediction']} ({result['confidence']:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
