{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a6206c",
   "metadata": {},
   "source": [
    "# Deepfake Voice Detection on Amazon SageMaker\n",
    "\n",
    "Deploy the fine-tuned Wav2Vec2 model for deepfake voice detection on Amazon SageMaker using a custom inference script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499075d",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Install required dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf414e",
   "metadata": {},
   "source": [
    "## Step 2: Download Model\n",
    "\n",
    "Clone the Deepfake model of your choice using `git-xet`:\n",
    "\n",
    "```bash\n",
    "# Make sure git-xet is installed (https://hf.co/docs/hub/git-xet)\n",
    "brew install git-xet\n",
    "git xet install\n",
    "\n",
    "git clone https://huggingface.co/garystafford/wav2vec2-deepfake-voice-detector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1336c70f",
   "metadata": {},
   "source": [
    "### Alternative: Using Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d10473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"garystafford/wav2vec2-deepfake-voice-detector\"\n",
    "snapshot_download(repo_id, local_dir=\"wav2vec2-deepfake-voice-detector\")\n",
    "\n",
    "print(f\"Model downloaded: {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c1eca",
   "metadata": {},
   "source": [
    "## Step 3: Package Model Artifacts\n",
    "\n",
    "Package the model and code directories and upload to S3.\n",
    "\n",
    "```txt\n",
    "model.tar.gz\n",
    "  â”œâ”€ model/\n",
    "  â”‚   â”œâ”€ config.json\n",
    "  â”‚   â”œâ”€ model.safetensors\n",
    "  â”‚   â””â”€ preprocessor_config.json\n",
    "  â””â”€ code/\n",
    "      â”œâ”€ inference.py\n",
    "      â””â”€ requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f29e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "python prepare_sagemaker_model.py --model-path wav2vec2-deepfake-voice-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various settings\n",
    "artifact_path = \"model.tar.gz\"\n",
    "key_prefix = \"wav2vec2-deepfake-voice-detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set an environment variable for your existing Sagemaker Execution Role ARN\n",
    "os.environ[\"SAGEMAKER_ROLE_ARN\"] = (\n",
    "    \"arn:aws:iam::676164205626:role/service-role/AmazonSageMaker-ExecutionRole-<your-role-id>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Zip up the model artifacts and code directories into a tar.gz file\n",
    "with tarfile.open(artifact_path, \"w:gz\") as tar:\n",
    "    tar.add(\"model\", arcname=os.path.basename(\"model\"))\n",
    "    tar.add(\"code\", arcname=os.path.basename(\"code\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002a3d6",
   "metadata": {},
   "source": [
    "### Validate Model Artifacts\n",
    "\n",
    "Before uploading and deploying, verify `model.tar.gz` contains `model/preprocessor_config.json`, `model/config.json`, and `model/model.safetensors`. Also confirm the packaged `code/inference.py` includes the latest model directory resolution logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b94cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model.tar.gz contents and validate required files\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "artifact_path = globals().get(\"artifact_path\", \"model.tar.gz\")\n",
    "\n",
    "assert os.path.isfile(artifact_path), f\"Missing artifact: {artifact_path}\"\n",
    "\n",
    "with tarfile.open(artifact_path, \"r:gz\") as tar:\n",
    "    names = tar.getnames()\n",
    "    print(\"Tar contents (first 20):\", names[:20])\n",
    "    required = [\n",
    "        \"model/preprocessor_config.json\",\n",
    "        \"model/config.json\",\n",
    "        \"model/model.safetensors\",\n",
    "        \"code/inference.py\",\n",
    "        \"code/requirements.txt\",\n",
    "    ]\n",
    "    missing = [p for p in required if p not in names]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"Missing required paths in tar: {missing}\")\n",
    "\n",
    "print(\"Artifact validation passed âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf55ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Create low-level clients\n",
    "session = boto3.Session()\n",
    "region = session.region_name or \"us-east-1\"\n",
    "sts = session.client(\"sts\", region_name=region)\n",
    "iam = session.client(\"iam\", region_name=region)\n",
    "sagemaker_client = session.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Emulate SageMaker SDK's default bucket convention if you like\n",
    "role_arn = os.environ.get(\"SAGEMAKER_ROLE_ARN\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "default_bucket = f\"sagemaker-{region}-{account_id}\"\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"RoleArn: {role_arn}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default S3 Bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "file_name = os.path.basename(artifact_path)\n",
    "s3_key = f\"{key_prefix}/{file_name}\"\n",
    "\n",
    "s3.upload_file(artifact_path, default_bucket, s3_key)\n",
    "\n",
    "model_s3_path = f\"s3://{default_bucket}/{s3_key}\"\n",
    "print(f\"Uploaded model artifact to: {model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff43f54",
   "metadata": {},
   "source": [
    "## Step 4: Deploy to SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d76c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.session import Session as SmSession\n",
    "\n",
    "# Use the same region/session as earlier\n",
    "sm_session = SmSession(boto_session=session)\n",
    "\n",
    "# Ensure container image URI matches the chosen region\n",
    "# https://gallery.ecr.aws/deep-learning-containers/pytorch-inference\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker-v1.56\"\n",
    "\n",
    "# Note: the current SageMaker handler returns the model's raw labels (e.g., 'real'/'fake')\n",
    "# and does not support label swapping or server-side trimming via env vars.\n",
    "container_env = {}\n",
    "\n",
    "# Endpoint/model name\n",
    "custom_endpoint_model_name = f\"{key_prefix}-\" + datetime.now(timezone.utc).strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\"\n",
    ")\n",
    "\n",
    "custom_model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_s3_path,\n",
    "    role=role_arn,\n",
    "    sagemaker_session=sm_session,\n",
    "    name=custom_endpoint_model_name,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    "    env=container_env,\n",
    ")\n",
    "\n",
    "instance_type = \"ml.g4dn.xlarge\"\n",
    "\n",
    "print(f\"Deploying to endpoint: {custom_endpoint_model_name}\")\n",
    "print(\"Container env:\", container_env)\n",
    "\n",
    "globals()[\"custom_endpoint_model_name\"] = custom_endpoint_model_name\n",
    "\n",
    "# Takes 7-11 minutes to deploy endpoint\n",
    "predictor = custom_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=custom_endpoint_model_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    container_startup_health_check_timeout=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76401db9",
   "metadata": {},
   "source": [
    "## Step 5: Real-time Inference\n",
    "\n",
    "Update the `endpoint_name` variable with your deployed endpoint name and run inference on local audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import base64\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Prefer the endpoint created by the deploy cell; otherwise use a known endpoint name.\n",
    "endpoint_name = \"<your-endpoint-name>\"  # replace with your endpoint name if needed\n",
    "assert (\n",
    "    endpoint_name and endpoint_name != \"<your-endpoint-name>\"\n",
    "), \"Set endpoint_name first\"\n",
    "\n",
    "\n",
    "def canonical_label(label: str | None) -> str | None:\n",
    "    if label is None:\n",
    "        return None\n",
    "    s = str(label).strip()\n",
    "    if not s:\n",
    "        return s\n",
    "    v = s.lower()\n",
    "    if v in {\"fake\", \"deepfake\", \"synthetic\"}:\n",
    "        return \"Deepfake\"\n",
    "    if v in {\"real\", \"bonafide\", \"bona-fide\", \"bona fide\", \"non-synthetic\"}:\n",
    "        return \"Real\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def canonicalize_probabilities(probs: dict | None) -> dict[str, float]:\n",
    "    out: dict[str, float] = {}\n",
    "    if not isinstance(probs, dict):\n",
    "        return out\n",
    "    for k, v in probs.items():\n",
    "        ck = canonical_label(k)\n",
    "        if ck in {\"Deepfake\", \"Real\"} and isinstance(v, (float, int)):\n",
    "            out[ck] = float(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def send_audio_to_sagemaker(audio_file: str) -> dict:\n",
    "    # Load and resample to 16 kHz on client\n",
    "    waveform, _ = librosa.load(audio_file, sr=16000, mono=True)\n",
    "    waveform = waveform.astype(\"float32\")\n",
    "\n",
    "    # Encode to base64 float32 PCM\n",
    "    audio_b64 = base64.b64encode(waveform.tobytes()).decode(\"utf-8\")\n",
    "    payload = {\"audio_base64\": audio_b64, \"sample_rate\": 16000}\n",
    "\n",
    "    # Send to endpoint\n",
    "    rt = boto3.client(\"sagemaker-runtime\")\n",
    "    resp = rt.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    return json.loads(resp[\"Body\"].read())\n",
    "\n",
    "\n",
    "audio_dir = \"audio_samples\"\n",
    "audio_files = [\n",
    "    os.path.join(audio_dir, file)\n",
    "    for file in os.listdir(audio_dir)\n",
    "    if file.endswith(\".flac\") or file.endswith(\".wav\") or file.endswith(\".mp3\")\n",
    "]\n",
    "\n",
    "print(\"Endpoint:\", endpoint_name)\n",
    "print()\n",
    "\n",
    "audio_files.sort()\n",
    "\n",
    "for path in audio_files:\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Missing:\", path)\n",
    "        continue\n",
    "    resp = send_audio_to_sagemaker(path)\n",
    "\n",
    "    raw_pred = resp.get(\"prediction\")\n",
    "    probs_raw = resp.get(\"probabilities\") or {}\n",
    "    probs = canonicalize_probabilities(probs_raw)\n",
    "\n",
    "    pred = canonical_label(raw_pred)\n",
    "    conf = resp.get(\"confidence\")\n",
    "    if conf is None and probs:\n",
    "        conf = max(probs.values())\n",
    "    if pred is None and probs:\n",
    "        pred = max(probs, key=probs.get)\n",
    "\n",
    "    pred_symbol = \"ðŸ”´\" if pred == \"Deepfake\" else \"ðŸŸ¢\"\n",
    "\n",
    "    print(f\"{pred_symbol} {path.split('\\\\')[-1]:30s} â†’ {pred:5s} ({conf:.1%})\")\n",
    "\n",
    "    if \"Deepfake\" in probs and \"Real\" in probs:\n",
    "        print(\n",
    "            f\"  probabilities: Deepfake={float(probs['Deepfake']):.8f}  Real={float(probs['Real']):.8f}\"\n",
    "        )\n",
    "    else:\n",
    "        # Show both so it's obvious what the endpoint returned vs what we canonicalized\n",
    "        print(\"  probabilities_raw:\", probs_raw)\n",
    "        print(\"  probabilities_canonical:\", probs)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
